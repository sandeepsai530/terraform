Linux:
du vs df

connect via ssh - ssh -i ~/downloads/vpn.pem ec2-user@ip-address

Topics to cover:
- why terraform? advantages
terraform commands:
terraform init - Initializes working directory(downloads required files like provider), its first cmd.
terraform plan - shows what changes terraform will make without applyig them.
terraform validate - checks whether the cnfiguration is syntactically valid. Terraform validate is included in terraform apply
terraform apply - Applies the changes to your infra. will prompt for approvals unless -auto-approve used.
terraform destroy - Tears down all resources defined in the current configuration.
terraform fmt - Formats .tf files to the canonical style.

.gitignore - files that not needed to be tracked by git can be placed under .gitignore folder

Manage terraform:
-----------------
Remote backend - store the statefile in S3 [backend.tf]
State locking - to avoid multiple users updating the statefile at the same time use DynamoDB

Mainly we need to remember four components while writing HCL scripts.
- provider block
- resource block
- variable block
- output block

Steps to consider for creating a EKS cluster usinf terraform(Explained by Abhishek Veeramalla)
- iam role, add policy and create cluster
- iam role, add policy and create nodegroup
Note: refer video by Abhishek Veeramalla from Udemy 41 session

variables:
-----------
syntax:
variable "<variable-name>" {
  type = ""
  default= ""
}

number,string,bool,map,list
Number of ways variables can be defined:
    - variables.tf 
    - terraform.tfvars
    - Inline with CLI (terraform apply -var="instance_type=t2.small")
    - ENV variables (export TF_VAR_instance_type=t2.medium)
    - command prompt
    - modules
    - Locals
      Ex: locals {
            app_name = "${var.project}-${var.environment}"
          }

  prefernce of variables:
  - command line (-var="instance_type=t2.small")
  - tfvars
  - env var
  - default values
  - user prompt/command prompt

if declared as below the n variable is considered as mandatory
variable "sg_id" {
}

if declared as below the n variable is considered as optional
variable "tags" {
  default = {}
}


- conditions -> expression ? "true-value : "false-value"
  Example: instance_type = var.Environment == "prod" ? "t2.medium" : "t2.micro"

- loops -> count based, for each, dynamic blocks
- functions - max, length, split, merge, join, lower, upper, file
** learn split vs join functions
- data sources -> query existing information
  Example:
  data "aws_ami" "joindevops" {
  most_recent = true
  owners = ["973714476881"]

  filter {
    name = "name"
    values = ["RHEL-9-DevOps-Practice"]
  }
}

- output
- locals -> store expressions in a variable
  Example:
  locals {
  ami_id = data.aws_ami.joindevops.id
}
- state and remote state with locking
- provisioners -> local-exec and remote-exec
- multiple environments -> tfvars

#conditions in terraform
please refer session-27 in joindevops
variable "instance_type" {
  default = "t2.micro" 
  validation {
    condition = contains(["t3.micro", "t3.small", "t3.medium"], var.instance_type)
    error_message = "valid values for instance type are: t3.small/t3.micro/t3.medium"
  }
}

Data sources:
-------------
-> data sources are used to query existing information from the provider
-> data sources in terraform are used to get information about resources external to terraform and uses them to set up your terraform resources.

Terraform taint:
----------------


types variables that we can pass:
1. command line -> -var "<instance_type=t2.micro" or -var "<var-name>=<var-value>"
2. tfvars
3. env var
4. default values
5. user prompt

conditions:
if else, when

if(expression) {
  this statement runs if expression is true
}
else {
  this statement runs if expression is false
}

expression ? "this runs if true" : "this runs if false"
example:  instance_type = var.environment == "prod" ? "t3.small" : "t2.micro"

loops:
1. count based loops
2. for each loops
3. dynamic blocks

- count lets you create multiple identical resources using a numeric index.
resource "aws_instance" "web" {
  count         = 3
  ami           = "ami-123456"
  instance_type = "t2.micro"
  tags = {
    Name = "web-${count.index}"
  }
}

- for_each is more flexible, allowing multiple resources based on a map or set, giving custom keys 
  for each resource.
variable "instances" {
  default = {
    frontend = "t2.micro"
    backend  = "t2.small"
  }
}

resource "aws_instance" "app" {
  for_each      = var.instances
  ami           = "ami-123456"
  instance_type = each.value
  tags = {
    Name = each.key
  }
}

- dynamic blocks let you programmatically generate nested blocks, useful when the number or content of 
  sub-blocks depends on variables or lists.
variable "ingress_rules" {
  default = [
    { from_port = 80,  to_port = 80,  cidr_blocks = ["0.0.0.0/0"] },
    { from_port = 443, to_port = 443, cidr_blocks = ["0.0.0.0/0"] }
  ]
}

resource "aws_security_group" "web_sg" {
  name = "web-sg"

  dynamic "ingress" {
    for_each = var.ingress_rules
    content {
      from_port   = ingress.value.from_port
      to_port     = ingress.value.to_port
      protocol    = "tcp"
      cidr_blocks = ingress.value.cidr_blocks
    }
  }
}

Count vs for-each:
count - when you need N identical resources.
for-each: when you have a collection of named or uniquely configured resources.
    When you want more control or to create resources based on a map or set of values, you use for_each.
    access each item using each.key and each.value.
    useful for creating multiple similar resources without repeating code.

interpolation: concating two variables with text

Output blocks: These are used to print the information. it will be used in module develeopment too.

locals:
-------
Locals are used to run expressions or functions and save the results to variables. refer below
session-25: 19mins
Example:
locals {
  ami_id = data.aws_ami.joindevops.id
}

LOCALS vs VARIABLES:
--------------------
locals are used to store expressions, it can even store simple key value pairs just like variables.
variables can't store expressions. variables can't refer other variable. locals can refer other locals or variables.
variables can be overriden. locals can't be overriden. 
Example for locals:
locals {
  instance_type = "t3.micro"
}

instance_type = local.instance_type

Example for variables:
variable "instance_type" {
  default = "t3.micro"
}

instance_type=var.instance_type

State Management:
-----------------
declared/desired infra -> .tf files. whatever user write in tf files, that is what user wants.
actual infra -> what terraform is creating in provider.

desired infra == actual infra

terraform.tfstate -> it is a file terraform creates to know what it is created. this is actual infra created by terraform.

someone changed the name of ec2 manually inside console.
terraform plan or apply
terraform reads state file and then compare that with actual infra using provider.

if you update tf files...
terraform.tfstate -> expense-dev-backend
compared with tf files -> expense-dev-backend-changed

if you update few parameters, resources will not be created again it will just update
but few parameters if you update, we are forced to recreate resources.

tfstate is very important file, we need to secure it.

clone terraform repo -> terraform apply
duplicate resources or errors

in collaboration environment we must maintain state file remotely.
locking is important, so that we can prevent parallel execution.

Dynamic blocks:
---------------
loops are used to create multiple resources.
dynamic blocks are used to create multiple blocks inside a resource.

Loops:
------
1-count based loops: use it to iterate lists
2-for each loop: use it to iterate maps
3-dynamic blocks

Terraform import:
----------------
Terraform import is used to bring existing infrastructure under Terraform management by updating the state file. It allows Terraform to track and manage resources created outside of Terraform.
Example:
Pre-requisite:
Bucket Name: my-existing-bucket (this should exist in the S3)
main.tf:
provider "aws" {
  region = "us-east-1"
}
# Define the S3 bucket resource
resource "aws_s3_bucket" "my_bucket" {
  bucket = "my-existing-bucket"
  # Optional: you can add more configurations after import
  acl    = "private"

  tags = {
    Environment = "Dev"
    Project     = "TerraformImportDemo"
  }
}

command: 
terraform init
terraform import aws_s3_bucket.my_bucket my-existing-bucket


provisioners:
-------------
provisioners are used to take some action either locally or remote when terraform created servers
Creation time:
By default, provisioners run when the resource they are defined within is created.
provisioners are only run during creation, not during update or anyother lifecycle.

destroy time:
if when = destroy is specified, the provisioner will run when the resource is defined within is destroyed
By default it takes when = creation time if nothing was mentioned
2 type of provisioners:
1-local-exec
2-remote-exec

we can use provisoners either creation time or destroy time.

local -> where terraform command is running that is local
remote -> inside the server created by terraform

when = destroy # terraform remote provisioners can be used either while creating or destroying.
# if nothing was mentioned then it will consider to eexcute the provisioner during the execution.

multiple environemnts using terraform:
--------------------------------------
1.tfvars -> used to override default values
2.workspaces
3.seperate codebase - maintain different repos for different environment.

its better to use seperate codebase as other two ways create confusion while creating the resources.

Disadvantages of tfvars/workspaces:
should be very careful, because same code to prod also, any mistake in dev causes confusion in prod.
should have full expertise, too much testing.

Advantages:
code reuse

Disadvantages with seperate codebase:
multiple repos to manage
we may be multiple employees
Code reuse can be achieved with the help of modules.

terraform modules:
variables, functions, ansible roles, locals.

modules: it is like functions, you can pass inputs you will get infra.

** In terraform modules, if you need to access variables then that has to be given in outputs.tf then it can be accessible.



DEV and PROD, remote state use

expense-dev-mysql -> mysql-dev.saisandeep-devops.xyz
expense-dev-backend -> backend-dev.saisandeep-devops.xyz
expense-dev-frontend -> frontend-dev.saisandeep-devops.xyz

expense-prod-mysql -> mysql-prod.saisandeep-devops.xyz
expense-prod-backend -> backend-prod.saisandeep-devops.xyz
expense-prod-frontend -> frontend-prod.saisandeep-devops.xyz

expense-dev or expense-prod

using TFVARS:
-------------
terraform init -reconfigure -backend-config=dev/backend.tf  --> for dev
terraform plan -var-file=dev/dev.tfvars
terraform apply -var-file=dev/dev.tfvars
terraform destroy -var-file=dev/dev.tfvars

terraform init -reconfigure -backend-config=prod/backend.tf  --> for prod
terraform plan -var-file=dev/dev.tfvars
terraform apply -var-file=prod/prod.tfvars
terraform destroy -var-file=prod/prod.tfvars

NAT Gateway:
If you want to enable egress internet to the servers in private subnet, you should create NAT gateway in
public subnet and provide route to private subnets
NAT gateway is for out going traffic

Bastion host:
when you try to connect private servers like backend and database, we need to use bastion host.
Bastion host is a secure, publicly accessible server that acts as a gateway for accessing private 
resources like EC2 instances inside a private subnet of a VPC.

Load Balancer:
ALB -> Listener -> Evaluate rules -> target group -> server

open source modules:
Advantages:
- we dont need to write code

Disadvantages:
- we are dependent on them
- whenever they update, we are forced to update

organisation module:
- we have to write complete code

Advantages:
- fully our control

DB vs RDS:
----------
DB:
- DB installation
- DB upgrades
- DB backups
- DB changes
- DB cluster setup
- DB restoration test

RDS:
- DB installation -> we just need to provide configuration
- DB upgrades - simple clicks
- DB snapshots
- DB cluster -> we can configure replication controllers, high available, storage scalable

what is null resource?
- it will not create any new resource, it is used to connect to the instances, copy the scripts, execute the scripts through provisioners. it has a trigger attribute to take actions when something is changed like instance id.

***OSI model:
----------
Application layer
Presentation layer
session layer
transport layer
network layer
data link layer
physical layer

-> when you hit www.google.com , what happens in the background?
Application layer:
------------------
application layer hits www.google.com - http/https
presentation - encryption
session - session, cookie details
NOTE: three layers together can be called as application layer

transport layer:
----------------
transport - port number
destination port - 80/443/8080/22
source port - system randomly allocates one port to get the resposne - ephemeral port
range: 32768-65535

Network layer:
--------------
Network - IP address
destination IP address - DNS resoultion
SourceIP - server IP address (private IP)
Source IP, source port, destination IP, destination port, data -> packet

Data link -> Mac address
source  mac - laptop mac address
destination mac - router mac address 
frame -> data pack and mac address

physical layer - ethernet, wifi

--------------------------------------------------------------------------------------
Docker vs VirtualMachines

Docker commands:
----------------
docker images
docker ps 
docker ps -a 
docker pull
docker create
docker start
docker rm 
docker exec -it <container-id> bash
docker logs <container-id>
docker inspect <container-id>
docker inspect <image-id>
docker login -u <username>
docker ps -a --no-trunc
docker build -t arg :1.0.0 --progress=plain . -> this provides complete output


**docker pull + docker create + docker start = docker run
docker run -d <image-name> -> images runs in the background as detached mode 
**docker run -d -p 80:80 nginx - > docker run -d -p <hostport>:<containerport> nginx
**host port is managed by us but container port is fixed

what is dockerfile?
It is used to create our custom images using instructions provided by docker.

CMD vs Entrypoint:
------------------
- CMD instructions can be overriden
- you cant override entrypoint -> ping google.com ping facebook.com. If you try to override     ENTRYPOINT it will not override, but it will append.
- For best results we can use CMD and ENTRYPOINT together.
- we can mention command in ENTRYPOINT, default options/inputs can be supplied through CMD. user can always override default options.
- only one CMD and one ENTRYPOINT should be used in Dockerfile.
@16:00 session-44 for better practical explanation.

USER -> set the user of container
WORKDIR -> set the working directory of container/image

ARG vs ENV:
-----------
- ENV variables can be accessed at the image building and in container also.
- ARG variables are only accessed inside image build, not in container.
- ARG can be the first instruction only to provide the version for base image. It cant be useful after FROM instruction.
@55:00 session-44 for detailed explanation

docker volumes:
---------------
session-46 @30:52

docker maintains the images as layers, each and every instruction is one layer. docker creates
1. intermediate container from instruction-1
2. docker runs 2nd instruction on top of IC-1. then docker saves this another layer.
3. docker saves this container as another image layer. create intermediate container out of it IC-2
4. now docker runs 3rd instruction in IC-2 container. docker saves this as another layer.
5. docker creates intermediate container from this layer as IC-3
Refer: session-47 @25:00

How do you optimise docker layers?
----------------------------------
1. less number of layers fater the builds, bcos number of intermediate containers are less.
    you can club multiple instructions into single instruction.
2. keep the frequently chnaging instructions at the bottom of Dockerfile.

Multi-stage builds:
-------------------
for example: Java
JDK - Java Development Kit
JRE - Java Runtime Environment

JDK > JRE
JRE is subset of JDK 

JDK = JRE + extra libraries

while installing some libraries, OS adds extra space to HD. we will take only that jar file output and copy it another Dockerfile where only jre runs.

we can have 2 dockerfiles one is for builder, we can copy the output from builder into 2nd dockerfile and run it, we can save some image space using this.

docker multi stage builds are primarily used to create smaller, more optimized images by seperating the build environment from the runtime environment. we can have one as builder and one as runner, copy the backend output from builder to runner. docker removes builder automatically.

what if underlying docker server crashes?
- we need to maintain multiple docker hosts. you need some orchestrator to manage all the docker hosts. docker swarm is the native orchestrator.

Kubernetes Architecture:
-  Kubernetes has 2 main planes
   Control plane(Master components) - responsible for cluster management & decisons.
   Data plane(Worker node components) - responsible for running application workloads.
   Control plane components:
   - API Server(kube-apiserver):
     Role: Central entry point to the cluster.
     Exposes Kubernetes REST API (all kubectl and SDKs talk to this).
     Handles authentication, authorization, admission control.
     Acts as the front-end of Kubernetes.

   - etcd:
     Role: Distributed key-value store.
     Stores all cluster state & configurations (Pods, Secrets, ConfigMaps, etc.).
     Needs to be backed up regularly.
     High availability is critical (usually runs in odd-number quorum). 

   - Scheduler(kube-scheduler):
     Role: Decides which node a Pod runs on.
     Scheduling decisions consider:
     Resource requirements (CPU/memory).
     Node labels, affinity/anti-affinity.
     Taints and tolerations.
     Topology constraints (zones, racks, etc.).  

   - Controller Manager (kube-controller manager):
     Role: Runs controllers that enforce desired state.
     Examples:
     Node Controller → detects node failures.
     ReplicaSet Controller → ensures the right number of Pods.
     Endpoints Controller → updates Services when Pods change.

   - Cloud Controller Manager (if using cloud providers)
     Role: Integrates with underlying cloud services.
     Examples:
     Create LoadBalancers when you create a Service of type LoadBalancer.
     Attach volumes to nodes.  

   Worker node components: 
   - kubelet:
     Role: Node agent that ensures containers are running.
     Talks to API Server → gets Pod specs and runs them using container runtime.
     Reports back node & pod status.

   - kube-proxy:
     Role: Handles networking and Service discovery.
     Maintains iptables/IPVS rules for load balancing traffic across Pods.
     Ensures Services (ClusterIP, NodePort, LoadBalancer) work properly. 

   - Container Runtime( Docker, containerd, CRI-O ):
     Role: Runs the actual containers inside Pods.
     Kubernetes itself doesn’t run containers; 
     it delegates this to the runtime via Container Runtime Interface (CRI). 

eksctl vs kubectl:
------------------
eksctl used for cluster creation , manage and delete.
kubectl used for interacting with cluster and pushing manifests.
aws configure used to setup your AWS CLI credentials and default settings.

alias ka="kubectl apply -f"

purpose of service:
loadbalancing and pod to pod communication through service
- ClusterIP: default, only works internally
- Nodeport: external requests
- LoadBalancer: only works with cloud providers. here service open classic LB.
LoadBalancer > NodePort > ClusterIP

NameSpace: 
- A Namespace in Kubernetes is a way to divide cluster resources between multiple users or teams.
- It helps organize and isolate resources like Pods, Services, and Deployments.
- Useful for multi-tenant environments, CI/CD pipelines, and managing resource quotas.
- it takes default namespace if nothing mentioned

Pod:
Pod is the smallest deplyable unit in k8. Pod can have multiple containers 1 or many.
multi-containers are useful in few applications like shipping logs through sidecar containers.

Static pods:
these are special type of pods managed directly by the kubelet on each node ratherthan through the kubernetes API server.
Example: ApiServer, kube-scheduler, control manager, ETCD etc.

Labels and selectors:
Labels are key-value pairs used to tag and organize resources like pods, deployments and services. They help organize and group resources based on criteria that make sense to you.
Example for labels:
environment: production
type: backend
tier: frontend
application: my-app

Selectors filter kubernetes objects based on their labels. This is incredibly useful for querying and managing a subset of objects that meet specific criteria.

common usage:
pods: kubectl get pods --selector app=my-app
Deployments: used to filter the pods managed by the deployment.
Services: filter the pods to which the service routes traffic.

Kubelet:
- The kubelet is an agent that runs on each node in the cluster.
- It ensures containers are running in a Pod by communicating with the control plane.
- It watches for PodSpecs and reports node status back to the master.

configMap:
configMap allow you to externalize configuration from your container images. you can store key-value pairs(like application settings, DB URL's, feature flags etc) and then inject them into pod as 
- environment variables
- command line arguments
- mounted volumes(files)
From ChatGpt:
- Used to store non-sensitive configuration data (key-value pairs, files, or environment variables).
- Example: application URLs, feature flags, environment names.
- Data is stored in plain text in etcd.
- Can be mounted into Pods as env variables or config files.

Secrets:
- Used to store sensitive data (passwords, tokens, certificates).
- Data is stored in etcd in base64 encoded form (not encrypted by default).
- Can be integrated with KMS or external secret managers for encryption.
- Access is tighter — RBAC can restrict access to Secrets.

Services:
Service allow different applications to communicate eachother internally or externally.
Types of Services:
- ClusterIP -> for internal access
- NodePort -> to access the application on a particular port
- LoadBalancer -> to access application on a domain name or IP address without using the port number
- External -> to use an external DNS for routing

LoadBalance > NodePort > ClusterIP

ReplicaSet vs Deployments:
ReplicaSets: Ensures a specified number of identical Pods are always running
  Drawback: No support for version updates or rollbacks.
Deployments: Manages ReplicaSets and enables declarative updates to Pods

In kubernetes, we have two types of resources which can be displayed using kubectl get api-resources
- namespace level(if NAMESPACED is true) 
- cluster level(if NAMESPACED is false)

Volumes:
Static and dynamic provisioning

PV, PVC and storage class

Pod -> PV -> PVC -> volume

access modes:
ReadWriteOnce, ReadOnlyMany, ReadWriteMany or ReadWriteOncePod

Life cycle policies:
Retain: Keeps the data after the PVC is deleted; manual cleanup required
Reclaim: (Deprecated) Attempts basic cleanup and makes PV available again
Delete: Deletes both the PVC and the actual underlying storage (e.g., AWS EBS volume)

steps to access persistent volume:
- install drivers
- give permissions to worker nodes(go to IAM role in workernode and add the access policy AmazonEBSCSIDriverPolicy)
- create volume
- create PV, PVC and claim through pod.
- EC2 and EBS volume should be in same AZ incase of EBS provisioning.

Storage class:
- this object is responsible for dynamic provisioning. it will create external storage and PV automatically.

this is achieved by below steps.
- install drivers
- give permissions to worker nodes(go to IAM role in workernode and add the access policy AmazonEBSCSIDriverPolicy)
- create storage class
k8s provides default storage class however we dont use it
kubectl get sc
default ReclaimPolicy: Delete , better use as Retain

EFS - Elastic File Sharing
- EBS should be in same AZ as in EC2, EFS can be anywhere in n/w.
- EBS speed is more compare to EFS.
- EBS is used for OS disk and databases. EFS can be used for file storages.
- EBS size is fixed. EFS will be scaled automatically.
- EFS is based on NFS protocol.
- EBS and EFS should be mounted to any instance.
- you can have any filesystem attached to EBS, but EFS use NFS. we cant change it.

EFS static:
- install drivers
- give permissions to worker nodes(go to IAM role in workernode and add the access policy AmazonEBSCSIDriverPolicy)
- create EFS
- open port 2049 on EFS to allow traffic from EC2 worker nodes. Refer: session 52: 39:00:00
- create PV, PVC and pod

EFS dynamic:
- install drivers
- create storage class
- give permissions

ImagePullPolicy:
- In Kubernetes, imagePullPolicy is a setting on a Pod/Container spec that tells Kubelet when to pull (download) the container image from the container registry (like Docker Hub, ECR, GCR, etc.).
1) Always
2) IfNotPresent
3) Never

Defaults & Special Behavior
- If imagePullPolicy is not specified, Kubernetes applies defaults:
- If the image tag is :latest → default is Always
- If the image tag is anything else (like v1.0.0) → default is IfNotPresent

StatefulSets:
- Deployment is for stateless applications, generally frontend and backend.
- StatefulSet is for stateful applications usually databases.
- StatefulSet will have headless service, deployment will not have headless service.
- PV, PVS are mandatory for statefulSet.
- pods in statefulSet create in orderly manner with static names. ex: statefulset-0, statefulset-1 etc.

Headless service:
- A headless service in Kubernetes is defined with clusterIP: None and is typically used when you want the client to directly resolve and communicate with individual pod IPs. This is especially useful in StatefulSets where each pod needs a stable network identity for coordination, like in Cassandra or Kafka clusters.

Deployment:
when you hit nslookup on servicename, you will get ClusterIP as address.

you need to attach headless service to statefulset. why?
what is headless service? headless service will have no clusterIP. It will be attached to StatefulSet.

why statefulset have same names, pods preserve their identity?
nginx-0 delete, statefulset creates immediately another pod nginx-0 and it should attach to its own storage through naming convention. persistentvolumeclaim/www-nginx-0.

Deployment vs StatefulSet:
- Deployment is for stateless applications, generally frontend and backend.
- statefulset is for stateful applications usually database.
- statefulset will have headless service, deployment will not have headless service.
- PV and PVC are mandatory for statefulset.
- pods in statefulset create in orderly manner with static names. statefulset-0, statefulset-1 etc

Liveness and Readiness probes:
- Liveness probes determine if a container is alive and should be restarted if unhealthy.
- readiness probes determine if a container is ready to serve traffic and temporarily remove it       from Service endpoints if not ready. 
- Liveness is for self-healing, readiness is for graceful traffic management. They can use HTTP, TCP, or exec commands and are configured with initial delays and periods for polling.

Autoscaling:
- Vertical scaling: 2CPU, 4GB, 20GB HDD --> 4CPU, 8GB, 50GB HDD
- Horizontal scaling: create another server with 2CPU, 4GB, 20GB HDD and add to LB.

kubernetes resources:
---------------------
- Namespace
- Pod
- ConfigMap
- Secret
- ReplicaSet
- Deployment
- Service
- PV
- PVC
- StatefulSet
- HPA
- Headless service
- Helm Charts
- resource requests and limits
- labels and selectors
- taints and tolerations
- affinity and anti-affinity
- pod affinity and pod anti affinity
- ingress controller and Ingress resource

HPA vs VPA:
- HPA scales applications out/in by changing the number of Pods, best for stateless, horizontally    scalable workloads. 
- VPA scales applications up/down by changing Pod CPU/memory resources, best for stateful or compute-heavy workloads. 
- In production, many teams use HPA + VPA together for full elasticity.

common errors in k8s:
- ErrImagePull - if node is not able to pull the image
- CrashLoopBackOff - container is unable to start
- Pending - worker node not available in that AZ, PVC is not bound to PV
- ContainerCreating - PV, PVC

Scheduling in kubernetes:
-------------------------
- Node selector
- taints and tolerations
- node affinity and anti-affinity
- pod affinity and anti-affinity

what is node selector?
- nodeSelector is the simplest form of node selection constraint in Kubernetes. It tells the scheduler to place a pod only on nodes that match a specific key-value label.
Ex: kubectl label nodes node1 disktype=ssd

Taints and Tolerations?
- Taints are applied to nodes to repel pods that do not tolerate them.
- Tolerations are applied to pods, allowing them to be scheduled on nodes with matching taints.
--> They work together to prevent certain pods from being scheduled on certain nodes unless explicitly allowed.
Options:
NoSchedule - Pods won’t be scheduled unless they tolerate the taint
PreferNoSchedule - Tries to avoid scheduling, but not strictly enforced
NoExecute	- Evicts existing pods if they don’t tolerate it, and blocks new pods too

Node affinity and Anti affinity?
- Node Affinity is a rule that tells the Kubernetes scheduler where a pod should or must run,   based on node labels. It allows more expressive control than simple nodeSelector.

- Node Anti-Affinity prevents pods from being scheduled on nodes with specific labels.
Used when you do not want pods to land on certain nodes.

Options:
- requiredDuringSchedulingIgnoredDuringExecution	Hard rule — pod won't be scheduled if condition not met
- preferredDuringSchedulingIgnoredDuringExecution	Soft rule — try to schedule on matching node, but not mandatory

Pod affinity and Anti affinity?
- Pod affinity is Schedule this pod on a node where certain other pods already exist
- Anti affinity is Avoid scheduling this pod on the same node as other specific pods.
Options:
requiredDuringSchedulingIgnoredDuringExecution	Hard rule — must meet condition
preferredDuringSchedulingIgnoredDuringExecution	Soft rule — try to meet condition



Taints and Tolerations:
if you taint the node, scheduler will not schedule the pods on that node.
If you apply toleration on a pod, it will be scheduled with matching nodes.
Effects for taint:
------------------
- NoSchedule -> its like an order
- PreferNoSchedule -> its a request
- NoExecute -> existing pods also get evicted

NoSchedule: Prevents new pods from being scheduled.
NoExecute: Also evicts existing pods that don't tolerate the taint.

How to add a taint?
kubectl taint nodes node1 special=true:Noschedule
this prevents any pod from being scheduled on node1 unless it tolerates this taint

Example:
kubectl taint nodes node1 environment=prod:NoSchedule

Pod with toleration:
--------------------
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  - key: "environment"
    operator: "Equal"
    value: "prod"
    effect: "NoSchedule"

this pod can run on the tainted node because it tolerates the environment=prod:NoSchedule taint

Remove a taint:
kubectl taint nodes node1 environment=prod:NoSchedule-

Taints applied at node level and tolerations applied at pod level.

Ingress resource:
-----------------
Ingress controller is used to provide external access to the applications running in kubernetes. we can set the routing rules through ingress resource either path based or host based. In EKS ingress resourcecan create ALB, Listener, Rule and target group. Ingress is attached to service, so it fetch the pods and add them to target group.

ServiceAccounts:
Service account is a non-human account, It is pod identity with which it can connect with api server and get access to external services.
when you create a namespace or every namespace will have default service account.
Service Account: default

Init Containers:
----------------
Init containers are used to setup the requirements for pod, for example backend pod can check whether database connections working fine or not.
Init Containers can fetch the secrets for pod before it starts.

InitContainers are special containers run before main container runs, we can use them to makesure dependent services are running fine before our main application starts and fetch the secrets from secret manager for main container. They always run to completion. you can run one or many init containers, they executed in sequence.

Init containers can do heavy lifting for main containers so that main container is less in size and limit attach surface of main container by not installing more tools in it

Kubernetes internal volumes:
emptyDir and hostPath
emptyDir -> pod temporary storage, accessible until pod lives. all containers in the pod can access init container. stores the secret in emptyDir and completes. then main container can access this.

DaemonSet:
Make sure pod replica runs on each node. monitoring purpose and logs collection, metric collection etc.

hostPath -> filesystem path in the underlying worker node. it is not secure, pods should not access host filesystems directly. But daemonset is only the exception for admins to collect logs and metrics.
